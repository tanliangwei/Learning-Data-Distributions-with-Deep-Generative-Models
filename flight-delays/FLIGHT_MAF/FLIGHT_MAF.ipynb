{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfx72GctB69e"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqyH8qz8CmnK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t) / (1 - t))\n",
    "\n",
    "def meters2lonlat(x, y):\n",
    "    semimajoraxis = 6378137.0\n",
    "    lon = x / semimajoraxis / 0.017453292519943295\n",
    "    t = math.exp(y / 3189068.5)\n",
    "    lat = math.asin((t - 1) / (t + 1)) / 0.017453292519943295\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVK4YY7AnK2n"
   },
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../processed_data.csv',delimiter=',', skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "\n",
    "def format_data(dataset, pick_up_scaler=None, drop_off_scaler = None ,save_scaler=True):\n",
    "    \n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # Handling of the coordinates\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "        lon = pick_up_c[i][0]\n",
    "        lat = pick_up_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        pick_up_c[i][0] = x\n",
    "        pick_up_c[i][1] = y\n",
    "    \n",
    "    if pick_up_scaler is None:\n",
    "        pick_up_scaler = preprocessing.StandardScaler()\n",
    "        pick_up_scaler = pick_up_scaler.fit(pick_up_c)\n",
    "    \n",
    "    pick_up_c = pick_up_scaler.transform(pick_up_c)\n",
    "    \n",
    "    for i, c in enumerate(drop_off_c):\n",
    "        lon = drop_off_c[i][0]\n",
    "        lat = drop_off_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        drop_off_c[i][0] = x\n",
    "        drop_off_c[i][1] = y\n",
    "    \n",
    "    if drop_off_scaler is None:\n",
    "        drop_off_scaler = preprocessing.StandardScaler()\n",
    "        drop_off_scaler = drop_off_scaler.fit(drop_off_c)\n",
    "    \n",
    "    drop_off_c = drop_off_scaler.transform(drop_off_c)\n",
    "    \n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(pick_up_scaler, open('pick_up_scaler.pkl', 'wb'))\n",
    "        dump(drop_off_scaler, open('drop_off_scaler.pkl', 'wb'))\n",
    "\n",
    "    final = np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)\n",
    "    return final\n",
    "\n",
    "dataset = format_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dZ_ZqQ-niOWy",
    "outputId": "16630e6b-5dfe-40b5-ecf6-f518f49d2e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.1.0\n",
      "tensorflow-probability:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization, ReLU, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print('tensorflow: ', tf.__version__)\n",
    "print('tensorflow-probability: ', tfp.__version__)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Case(Enum):\n",
    "    sampling = 1\n",
    "    density_estimation = 2\n",
    "\n",
    "class Made(tfk.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of a Masked Autoencoder for Distribution Estimation (MADE) [Germain et al. (2015)].\n",
    "    The existing TensorFlow bijector \"AutoregressiveNetwork\" is used. The output is reshaped to output one shift vector\n",
    "    and one log_scale vector.\n",
    "\n",
    "    :param params: Python integer specifying the number of parameters to output per input.\n",
    "    :param event_shape: Python list-like of positive integers (or a single int), specifying the shape of the input to this layer, which is also the event_shape of the distribution parameterized by this layer. Currently only rank-1 shapes are supported. That is, event_shape must be a single integer. If not specified, the event shape is inferred when this layer is first called or built.\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "    :param activation: An activation function. See tf.keras.layers.Dense. Default: None.\n",
    "    :param use_bias: Whether or not the dense layers constructed in this layer should have a bias term. See tf.keras.layers.Dense. Default: True.\n",
    "    :param kernel_regularizer: Regularizer function applied to the Dense kernel weight matrices. Default: None.\n",
    "    :param bias_regularizer: Regularizer function applied to the Dense bias weight vectors. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, event_shape=None, hidden_units=None, activation=None, use_bias=True,\n",
    "                 kernel_regularizer=None, bias_regularizer=None):\n",
    "\n",
    "        super(Made, self).__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.event_shape = event_shape\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        self.network = tfb.AutoregressiveNetwork(params=params, event_shape=event_shape, hidden_units=hidden_units,\n",
    "                                                 activation=activation, use_bias=use_bias, kernel_regularizer=kernel_regularizer, \n",
    "                                                 bias_regularizer=bias_regularizer)\n",
    "\n",
    "    def call(self, x):\n",
    "        shift, log_scale = tf.unstack(self.network(x), num=2, axis=-1)\n",
    "\n",
    "        return shift, tf.math.tanh(log_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "s0EpDb2rixJS",
    "outputId": "fc4e9312-00e9-4eb9-8239-a29786866ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tanliangwei/Desktop/fyp_repo/tf_1/lib/python3.6/site-packages/tensorflow_probability/python/bijectors/masked_autoregressive.py:310: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n"
     ]
    }
   ],
   "source": [
    "hidden_shape = [100, 100]  # hidden shape for MADE network of MAF\n",
    "layers = 12  # number of layers of the flow\n",
    "event_shape=[6]\n",
    "\n",
    "base_dist = tfd.Normal(loc=0.0, scale=1.0)  # specify base distribution\n",
    "\n",
    "bijectors = []\n",
    "for i in range(0, layers):\n",
    "    bijectors.append(tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn = Made(params=2, event_shape=event_shape, hidden_units=hidden_shape, activation=\"relu\")))\n",
    "    bijectors.append(tfb.Permute(permutation=[5,4,3,2,1,0]))  # data permutation after layers of MAF\n",
    "    \n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_maf')\n",
    "\n",
    "maf = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector,\n",
    "    event_shape=event_shape\n",
    ")\n",
    "\n",
    "# initialize flow\n",
    "samples = maf.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTG7WFpSmmQL"
   },
   "outputs": [],
   "source": [
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(5e3)  # maximum number of epochs of the training\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAfkJRh-mnGQ"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
    "\n",
    "x_ = tf.keras.layers.Input(shape=event_shape, dtype=tf.float32)\n",
    "log_prob_ = maf.log_prob(x_)\n",
    "model = tf.keras.Model(x_, log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqZHDr1Loa-F"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss=lambda _, log_prob: -log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Checkpoint (Will save model at every Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SF9uXEt6oo6d"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "weight_file = 'checkpoint/cp.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "CWW-n9mZpxVV",
    "outputId": "58eab50e-9225-4429-ce16-d28c84407fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10042/10042 [==============================] - 215s 21ms/step - loss: 7.9996\n",
      "10042/10042 [==============================] - 214s 21ms/step - loss: 7.8294\n",
      "10042/10042 [==============================] - 214s 21ms/step - loss: 7.7864\n",
      "10042/10042 [==============================] - 213s 21ms/step - loss: 7.9652\n",
      "10042/10042 [==============================] - 214s 21ms/step - loss: 7.8224\n"
     ]
    }
   ],
   "source": [
    "for i in range(5): # adjust from 5 to any number of epochs u want to train\n",
    "  model.load_weights(weight_file)\n",
    "  model.fit(x=dataset,\n",
    "            y=np.zeros((dataset.shape[0], 0), dtype=np.float32),\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=dataset.shape[0] // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=True)\n",
    "  model.save_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWLPuKFuuVDz"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"MAF_12_100_100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmcYetC7Ds-o"
   },
   "outputs": [],
   "source": [
    "output = maf.sample(100000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ayb3AGfHD1Mc"
   },
   "outputs": [],
   "source": [
    "def reconstruct(predictions):\n",
    "    \n",
    "    # split the output first\n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # recover scaler\n",
    "    pick_up_scaler = load(open('pick_up_scaler.pkl', 'rb'))\n",
    "    drop_off_scaler = load(open('drop_off_scaler.pkl', 'rb'))\n",
    "    pick_up_c = pick_up_scaler.inverse_transform(pick_up_c)\n",
    "    drop_off_c = drop_off_scaler.inverse_transform(drop_off_c)\n",
    "\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = pick_up_c[i][0]\n",
    "      y = pick_up_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      pick_up_c[i][0] = lon\n",
    "      pick_up_c[i][1] = lat\n",
    "    \n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = drop_off_c[i][0]\n",
    "      y = drop_off_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      drop_off_c[i][0] = lon\n",
    "      drop_off_c[i][1] = lat\n",
    "    \n",
    "    return np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Fca3K2cHZtD"
   },
   "outputs": [],
   "source": [
    "samples = reconstruct(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Sp-lrU6Hp49"
   },
   "outputs": [],
   "source": [
    "file_name = '100000_samples_MAF_12_100_100' + '.csv'\n",
    "np.savetxt(file_name, samples, delimiter = ',', header='origin_longitude, origin_latitude, destination_longitude, destination_latitude, depature_delay, arrival_delay' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyWIiBakI9AG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "FLIGHT_MAF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
