{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40w5QQEBMJ01"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WVWys12NwuL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t) / (1 - t))\n",
    "\n",
    "def meters2lonlat(x, y):\n",
    "    semimajoraxis = 6378137.0\n",
    "    lon = x / semimajoraxis / 0.017453292519943295\n",
    "    t = math.exp(y / 3189068.5)\n",
    "    lat = math.asin((t - 1) / (t + 1)) / 0.017453292519943295\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9zGqOzTN0Ld"
   },
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../processed_data.csv',delimiter=',', skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "\n",
    "def format_data(dataset, pick_up_scaler=None, drop_off_scaler = None ,save_scaler=True):\n",
    "    \n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # Handling of the coordinates\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "        lon = pick_up_c[i][0]\n",
    "        lat = pick_up_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        pick_up_c[i][0] = x\n",
    "        pick_up_c[i][1] = y\n",
    "    \n",
    "    if pick_up_scaler is None:\n",
    "        pick_up_scaler = preprocessing.StandardScaler()\n",
    "        pick_up_scaler = pick_up_scaler.fit(pick_up_c)\n",
    "    \n",
    "    pick_up_c = pick_up_scaler.transform(pick_up_c)\n",
    "    \n",
    "    for i, c in enumerate(drop_off_c):\n",
    "        lon = drop_off_c[i][0]\n",
    "        lat = drop_off_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        drop_off_c[i][0] = x\n",
    "        drop_off_c[i][1] = y\n",
    "    \n",
    "    if drop_off_scaler is None:\n",
    "        drop_off_scaler = preprocessing.StandardScaler()\n",
    "        drop_off_scaler = drop_off_scaler.fit(drop_off_c)\n",
    "    \n",
    "    drop_off_c = drop_off_scaler.transform(drop_off_c)\n",
    "    \n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(pick_up_scaler, open('pick_up_scaler.pkl', 'wb'))\n",
    "        dump(drop_off_scaler, open('drop_off_scaler.pkl', 'wb'))\n",
    "\n",
    "    final = np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)\n",
    "    return final\n",
    "\n",
    "dataset = format_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iJsjGeFkMJ6Q",
    "outputId": "8e4f63c5-a90f-4156-d910-89d340e4e1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.2.0-rc2\n",
      "tensorflow-probability:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization, ReLU, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print('tensorflow: ', tf.__version__)\n",
    "print('tensorflow-probability: ', tfp.__version__)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Case(Enum):\n",
    "    sampling = 1\n",
    "    density_estimation = 2\n",
    "\n",
    "class NN(Layer):\n",
    "    \"\"\"\n",
    "    Neural Network Architecture for calcualting s and t for Real-NVP\n",
    "    \n",
    "    :param input_shape: shape of the data coming in the layer\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "    :param activation: Activation of the hidden units\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_hidden=[512, 512], activation=\"relu\"):\n",
    "        super(NN, self).__init__()\n",
    "        layer_list = []\n",
    "        for i, hidden in enumerate(n_hidden):\n",
    "            layer_list.append(Dense(hidden, activation=activation))\n",
    "        self.layer_list = layer_list\n",
    "        self.log_s_layer = Dense(input_shape, activation=\"tanh\", name='log_s')\n",
    "        self.t_layer = Dense(input_shape, name='t')\n",
    "\n",
    "    def call(self, x):\n",
    "        y = x\n",
    "        for layer in self.layer_list:\n",
    "            y = layer(y)\n",
    "        log_s = self.log_s_layer(y)\n",
    "        t = self.t_layer(y)\n",
    "        return log_s, t\n",
    "\n",
    "class RealNVP(tfb.Bijector):\n",
    "    \"\"\"\n",
    "    Implementation of a Real-NVP for Denisty Estimation. L. Dinh “Density estimation using Real NVP,” 2016.\n",
    "    This implementation only works for 1D arrays.\n",
    "    :param input_shape: shape of the data coming in the layer\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, n_hidden=[512, 512], forward_min_event_ndims=1, validate_args: bool = False, name=\"real_nvp\"):\n",
    "        super(RealNVP, self).__init__(\n",
    "            validate_args=validate_args, forward_min_event_ndims=forward_min_event_ndims, name=name\n",
    "        )\n",
    "\n",
    "        assert input_shape % 2 == 0\n",
    "        input_shape = input_shape // 2\n",
    "        nn_layer = NN(input_shape, n_hidden)\n",
    "        x = tf.keras.Input(input_shape)\n",
    "        log_s, t = nn_layer(x)\n",
    "        self.nn = Model(x, [log_s, t])\n",
    "        \n",
    "    def _bijector_fn(self, x):\n",
    "        log_s, t = self.nn(x)\n",
    "        return tfb.affine_scalar.AffineScalar(shift=t, log_scale=log_s)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
    "        y_b = x_b\n",
    "        y_a = self._bijector_fn(x_b).forward(x_a)\n",
    "        y = tf.concat([y_a, y_b], axis=-1)\n",
    "        return y\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
    "        x_b = y_b\n",
    "        x_a = self._bijector_fn(y_b).inverse(y_a)\n",
    "        x = tf.concat([x_a, x_b], axis=-1)\n",
    "        return x\n",
    "\n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
    "        return self._bijector_fn(x_b).forward_log_det_jacobian(x_a, event_ndims=1)\n",
    "    \n",
    "    def _inverse_log_det_jacobian(self, y):\n",
    "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
    "        return self._bijector_fn(y_b).inverse_log_det_jacobian(y_a, event_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCvWxSDNOvcg"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layers = 12\n",
    "# base_lr = 1e-3\n",
    "# end_lr = 1e-4\n",
    "# max_epochs = int(100)\n",
    "shape = [128, 128]\n",
    "exp_number = 1\n",
    "input_shape = 6\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "event_shape = [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOvfYzKMOaak"
   },
   "outputs": [],
   "source": [
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "bijectors = []\n",
    "alpha = 1e-3\n",
    "\n",
    "for i in range(layers):\n",
    "    bijectors.append(tfb.BatchNormalization())\n",
    "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
    "    bijectors.append(tfp.bijectors.Permute(permutation))\n",
    "\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = len(flow.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6OeXZwdP7u4"
   },
   "outputs": [],
   "source": [
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(5e3)  # maximum number of epochs of the training\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "GRby0dMrSz1B",
    "outputId": "b41bb620-65a2-4645-94d4-082caa9d90d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-166a63a340c6>:72: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/bijectors/batch_normalization.py:207: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
    "\n",
    "x_ = tf.keras.layers.Input(shape=event_shape, dtype=tf.float32)\n",
    "log_prob_ = flow.log_prob(x_)\n",
    "model = tf.keras.Model(x_, log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNwEpiz7Vwcc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam() ,\n",
    "              loss=lambda _, log_prob: -log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Checkpoints (Will Save Model After Every Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKIR7DukS0XH"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "weight_file = './checkpoint/cp.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-OHsysMCUY7M",
    "outputId": "cdd9723d-7d17-4c56-f26c-85564c07dd37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10042/10042 [==============================] - 365s 36ms/step - loss: -234.0764\n",
      "10042/10042 [==============================] - 359s 36ms/step - loss: -249.3558\n",
      "10042/10042 [==============================] - 355s 35ms/step - loss: -249.4613\n",
      "10042/10042 [==============================] - 356s 35ms/step - loss: -249.5400\n",
      "10042/10042 [==============================] - 360s 36ms/step - loss: -249.6097\n",
      "10042/10042 [==============================] - 362s 36ms/step - loss: -249.6554\n",
      "10042/10042 [==============================] - 368s 37ms/step - loss: -249.6922\n",
      "10042/10042 [==============================] - 367s 37ms/step - loss: -249.7236\n",
      "10042/10042 [==============================] - 368s 37ms/step - loss: -249.7531\n",
      "10042/10042 [==============================] - 366s 36ms/step - loss: -249.7811\n",
      "10042/10042 [==============================] - 353s 35ms/step - loss: -249.8069\n",
      "10042/10042 [==============================] - 351s 35ms/step - loss: -249.8274\n",
      "10042/10042 [==============================] - 352s 35ms/step - loss: -249.8418\n",
      "10042/10042 [==============================] - 351s 35ms/step - loss: -249.8496\n",
      "10042/10042 [==============================] - 357s 36ms/step - loss: -249.8820\n",
      "10042/10042 [==============================] - 364s 36ms/step - loss: -249.8945\n",
      "10042/10042 [==============================] - 368s 37ms/step - loss: -249.8802\n",
      "10042/10042 [==============================] - 361s 36ms/step - loss: -249.8919\n",
      "10042/10042 [==============================] - 363s 36ms/step - loss: -249.9030\n",
      "10042/10042 [==============================] - 350s 35ms/step - loss: -249.9175\n",
      "10042/10042 [==============================] - 335s 33ms/step - loss: -249.9203\n",
      "10042/10042 [==============================] - 328s 33ms/step - loss: -249.9319\n",
      "10042/10042 [==============================] - 337s 34ms/step - loss: -249.9276\n",
      "10042/10042 [==============================] - 331s 33ms/step - loss: -249.9415\n",
      "10042/10042 [==============================] - 338s 34ms/step - loss: -249.9449\n",
      "10042/10042 [==============================] - 351s 35ms/step - loss: -249.9551\n",
      "10042/10042 [==============================] - 355s 35ms/step - loss: -249.9609\n",
      "10042/10042 [==============================] - 353s 35ms/step - loss: -249.9670\n",
      "10042/10042 [==============================] - 360s 36ms/step - loss: -249.9734\n",
      "10042/10042 [==============================] - 348s 35ms/step - loss: -249.9766\n",
      "10042/10042 [==============================] - 341s 34ms/step - loss: -249.9860\n",
      "10042/10042 [==============================] - 343s 34ms/step - loss: -249.9846\n",
      "10042/10042 [==============================] - 330s 33ms/step - loss: -249.9885\n",
      "10042/10042 [==============================] - 326s 32ms/step - loss: -249.9912\n",
      "10042/10042 [==============================] - 328s 33ms/step - loss: -249.9514\n",
      "10042/10042 [==============================] - 335s 33ms/step - loss: -249.9997\n",
      "10042/10042 [==============================] - 337s 34ms/step - loss: -250.0071\n",
      "10042/10042 [==============================] - 337s 34ms/step - loss: -250.0084\n",
      "10042/10042 [==============================] - 338s 34ms/step - loss: -250.0038\n",
      "10042/10042 [==============================] - 338s 34ms/step - loss: -250.0201\n",
      "10042/10042 [==============================] - 336s 33ms/step - loss: -250.0242\n",
      "10042/10042 [==============================] - 337s 34ms/step - loss: -250.0218\n",
      "10042/10042 [==============================] - 331s 33ms/step - loss: -249.9245\n",
      " 6141/10042 [=================>............] - ETA: 2:06 - loss: -250.0242"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-547d0215b7d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             verbose=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "  model.load_weights(weight_file)\n",
    "  model.fit(x=dataset,\n",
    "            y=np.zeros((dataset.shape[0], 0), dtype=np.float32),\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=dataset.shape[0] // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=True)\n",
    "  model.save_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TzQhZTTU3Sj"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"RNVP_12_128_128.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwNcq369U_D2"
   },
   "outputs": [],
   "source": [
    "output = flow.sample(100000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wncLFKAQVB6f"
   },
   "outputs": [],
   "source": [
    "def reconstruct(predictions):\n",
    "    \n",
    "    # split the output first\n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # recover scaler\n",
    "    pick_up_scaler = load(open('pick_up_scaler.pkl', 'rb'))\n",
    "    drop_off_scaler = load(open('drop_off_scaler.pkl', 'rb'))\n",
    "    pick_up_c = pick_up_scaler.inverse_transform(pick_up_c)\n",
    "    drop_off_c = drop_off_scaler.inverse_transform(drop_off_c)\n",
    "\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = pick_up_c[i][0]\n",
    "      y = pick_up_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      pick_up_c[i][0] = lon\n",
    "      pick_up_c[i][1] = lat\n",
    "    \n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = drop_off_c[i][0]\n",
    "      y = drop_off_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      drop_off_c[i][0] = lon\n",
    "      drop_off_c[i][1] = lat\n",
    "    \n",
    "    return np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDDYJY6VVKLI"
   },
   "outputs": [],
   "source": [
    "samples = reconstruct(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arxV_BtoVMNc"
   },
   "outputs": [],
   "source": [
    "file_name = '100000_samples_RNVP_12_128_128' + '.csv'\n",
    "np.savetxt(file_name, samples, delimiter = ',', header='origin_longitude, origin_latitude, destination_longitude, destination_latitude, depature_delay, arrival_delay' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1w3U15vB3IDs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "FLIGHT_RNVP_12.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
