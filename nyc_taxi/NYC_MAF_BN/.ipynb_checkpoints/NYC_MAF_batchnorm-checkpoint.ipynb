{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfx72GctB69e"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqyH8qz8CmnK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t) / (1 - t))\n",
    "\n",
    "def meters2lonlat(x, y):\n",
    "    semimajoraxis = 6378137.0\n",
    "    lon = x / semimajoraxis / 0.017453292519943295\n",
    "    t = math.exp(y / 3189068.5)\n",
    "    lat = math.asin((t - 1) / (t + 1)) / 0.017453292519943295\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVK4YY7AnK2n"
   },
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../processed_nyc_train.csv',delimiter=',', skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "\n",
    "def format_data(dataset, pick_up_scaler=None, drop_off_scaler = None ,save_scaler=True):\n",
    "    \n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # Handling of the coordinates\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "        lon = pick_up_c[i][0]\n",
    "        lat = pick_up_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        pick_up_c[i][0] = x\n",
    "        pick_up_c[i][1] = y\n",
    "    \n",
    "    if pick_up_scaler is None:\n",
    "        pick_up_scaler = preprocessing.StandardScaler()\n",
    "        pick_up_scaler = pick_up_scaler.fit(pick_up_c)\n",
    "    \n",
    "    pick_up_c = pick_up_scaler.transform(pick_up_c)\n",
    "    \n",
    "    for i, c in enumerate(drop_off_c):\n",
    "        lon = drop_off_c[i][0]\n",
    "        lat = drop_off_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        drop_off_c[i][0] = x\n",
    "        drop_off_c[i][1] = y\n",
    "    \n",
    "    if drop_off_scaler is None:\n",
    "        drop_off_scaler = preprocessing.StandardScaler()\n",
    "        drop_off_scaler = drop_off_scaler.fit(drop_off_c)\n",
    "    \n",
    "    drop_off_c = drop_off_scaler.transform(drop_off_c)\n",
    "    \n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(pick_up_scaler, open('pick_up_scaler.pkl', 'wb'))\n",
    "        dump(drop_off_scaler, open('drop_off_scaler.pkl', 'wb'))\n",
    "\n",
    "    final = np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)\n",
    "    return final\n",
    "\n",
    "dataset = format_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dZ_ZqQ-niOWy",
    "outputId": "ff389425-5a13-4e40-88cf-78ba1da31e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.2.0-rc2\n",
      "tensorflow-probability:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization, ReLU, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print('tensorflow: ', tf.__version__)\n",
    "print('tensorflow-probability: ', tfp.__version__)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Case(Enum):\n",
    "    sampling = 1\n",
    "    density_estimation = 2\n",
    "\n",
    "class Made(tfk.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of a Masked Autoencoder for Distribution Estimation (MADE) [Germain et al. (2015)].\n",
    "    The existing TensorFlow bijector \"AutoregressiveNetwork\" is used. The output is reshaped to output one shift vector\n",
    "    and one log_scale vector.\n",
    "\n",
    "    :param params: Python integer specifying the number of parameters to output per input.\n",
    "    :param event_shape: Python list-like of positive integers (or a single int), specifying the shape of the input to this layer, which is also the event_shape of the distribution parameterized by this layer. Currently only rank-1 shapes are supported. That is, event_shape must be a single integer. If not specified, the event shape is inferred when this layer is first called or built.\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "    :param activation: An activation function. See tf.keras.layers.Dense. Default: None.\n",
    "    :param use_bias: Whether or not the dense layers constructed in this layer should have a bias term. See tf.keras.layers.Dense. Default: True.\n",
    "    :param kernel_regularizer: Regularizer function applied to the Dense kernel weight matrices. Default: None.\n",
    "    :param bias_regularizer: Regularizer function applied to the Dense bias weight vectors. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, event_shape=None, hidden_units=None, activation=None, use_bias=True,\n",
    "                 kernel_regularizer=None, bias_regularizer=None):\n",
    "\n",
    "        super(Made, self).__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.event_shape = event_shape\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        self.network = tfb.AutoregressiveNetwork(params=params, event_shape=event_shape, hidden_units=hidden_units,\n",
    "                                                 activation=activation, use_bias=use_bias, kernel_regularizer=kernel_regularizer, \n",
    "                                                 bias_regularizer=bias_regularizer)\n",
    "\n",
    "    def call(self, x):\n",
    "        shift, log_scale = tf.unstack(self.network(x), num=2, axis=-1)\n",
    "\n",
    "        return shift, tf.math.tanh(log_scale)\n",
    "\n",
    "class BatchNorm(tfb.Bijector):\n",
    "    \"\"\"\n",
    "    Implementation of a Batch Normalization layer for use in normalizing flows according to [Papamakarios et al. (2017)].\n",
    "    The moving average of the layer statistics is adapted from [Dinh et al. (2016)].\n",
    "    :param eps: Hyperparameter that ensures numerical stability, if any of the elements of v is near zero.\n",
    "    :param decay: Weight for the update of the moving average, e.g. avg = (1-decay)*avg + decay*new_value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eps=1e-5, decay=0.95, validate_args=False, name=\"batch_norm\"):\n",
    "        super(BatchNorm, self).__init__(\n",
    "            forward_min_event_ndims=1,\n",
    "            inverse_min_event_ndims=1,\n",
    "            validate_args=validate_args,\n",
    "            name=name)\n",
    "\n",
    "        self._vars_created = False\n",
    "        self.eps = eps\n",
    "        self.decay = decay\n",
    "\n",
    "    def _create_vars(self, x):\n",
    "        # account for 1xd and dx1 vectors\n",
    "        if len(x.get_shape()) == 1:\n",
    "            n = x.get_shape().as_list()[0]\n",
    "        if len(x.get_shape()) == 2: \n",
    "            n = x.get_shape().as_list()[1]\n",
    "\n",
    "        self.beta = tf.compat.v1.get_variable('beta', [1, n], dtype=tf.float32)\n",
    "        self.gamma = tf.compat.v1.get_variable('gamma', [1, n], dtype=tf.float32)\n",
    "        self.train_m = tf.compat.v1.get_variable(\n",
    "            'mean', [1, n], dtype=tf.float32, trainable=False)\n",
    "        self.train_v = tf.compat.v1.get_variable(\n",
    "            'var', [1, n], dtype=tf.float32, trainable=False)\n",
    "\n",
    "        self._vars_created = True\n",
    "\n",
    "    def _forward(self, u):\n",
    "        if not self._vars_created:\n",
    "            self._create_vars(u)\n",
    "        return (u - self.beta) * tf.exp(-self.gamma) * tf.sqrt(self.train_v + self.eps) + self.train_m\n",
    "\n",
    "    def _inverse(self, x):\n",
    "        # Eq. 22 of [Papamakarios et al. (2017)]. Called during training of a normalizing flow.\n",
    "        if not self._vars_created:\n",
    "            self._create_vars(x)\n",
    "\n",
    "        # statistics of current minibatch\n",
    "        m, v = tf.nn.moments(x, axes=[0], keepdims=True)\n",
    "        \n",
    "        # update train statistics via exponential moving average\n",
    "        self.train_v.assign_sub(self.decay * (self.train_v - v))\n",
    "        self.train_m.assign_sub(self.decay * (self.train_m - m))\n",
    "\n",
    "        # normalize using current minibatch statistics, followed by BN scale and shift\n",
    "        return (x - m) * 1. / tf.sqrt(v + self.eps) * tf.exp(self.gamma) + self.beta\n",
    "\n",
    "    def _inverse_log_det_jacobian(self, x):\n",
    "        # at training time, the log_det_jacobian is computed from statistics of the\n",
    "        # current minibatch.\n",
    "        if not self._vars_created:\n",
    "            self._create_vars(x)\n",
    "            \n",
    "        _, v = tf.nn.moments(x, axes=[0], keepdims=True)\n",
    "        abs_log_det_J_inv = tf.reduce_sum(\n",
    "            self.gamma - .5 * tf.math.log(v + self.eps))\n",
    "        return abs_log_det_J_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s0EpDb2rixJS"
   },
   "outputs": [],
   "source": [
    "hidden_shape = [100, 100]  # hidden shape for MADE network of MAF\n",
    "layers = 12  # number of layers of the flow\n",
    "event_shape=[6]\n",
    "\n",
    "base_dist = tfd.Normal(loc=0.0, scale=1.0)  # specify base distribution\n",
    "\n",
    "# According to [Papamakarios et al. (2017)]:\n",
    "# BatchNorm between the last autoregressive layer and the base distribution, and every two autoregressive layers\n",
    "bijectors = []\n",
    "bijectors.append(BatchNorm(eps=10e-5, decay=0.95))\n",
    "for i in range(0, layers):\n",
    "    bijectors.append(tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn = Made(params=2, event_shape=event_shape, hidden_units=hidden_shape, activation=\"relu\")))\n",
    "    bijectors.append(tfb.Permute(permutation=[5,4,3,2,1,0]))  # data permutation after layers of MAF\n",
    "\n",
    "    # add BatchNorm every two layers\n",
    "    if (i+1) % int(2) == 0:\n",
    "        bijectors.append(BatchNorm(eps=10e-5, decay=0.95))\n",
    "    \n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_maf')\n",
    "\n",
    "maf = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector,\n",
    "    event_shape=event_shape\n",
    ")\n",
    "\n",
    "# initialize flow\n",
    "# samples = maf.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTG7WFpSmmQL"
   },
   "outputs": [],
   "source": [
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(5e3)  # maximum number of epochs of the training\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "WAfkJRh-mnGQ",
    "outputId": "a751004b-f209-4bd2-afd2-b6317f5f27cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/bijectors/masked_autoregressive.py:310: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
    "\n",
    "x_ = tf.keras.layers.Input(shape=event_shape, dtype=tf.float32)\n",
    "log_prob_ = maf.log_prob(x_)\n",
    "model = tf.keras.Model(x_, log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqZHDr1Loa-F"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss=lambda _, log_prob: -log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Checkpoints (Will Save Model After Every Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SF9uXEt6oo6d"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "weight_file = './checkpoint/cp.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "CWW-n9mZpxVV",
    "outputId": "763ba598-0e36-4a1f-be9d-0e04f902ae6c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6b5b942761cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# change to desired number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   model.fit(x=dataset,\n\u001b[1;32m      4\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(50): # change to desired number of epochs\n",
    "  model.load_weights(weight_file)\n",
    "  model.fit(x=dataset,\n",
    "            y=np.zeros((dataset.shape[0], 0), dtype=np.float32),\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=dataset.shape[0] // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=True)\n",
    "  model.save_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWLPuKFuuVDz"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"MAF_12_100_100_batchnorm.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmcYetC7Ds-o"
   },
   "outputs": [],
   "source": [
    "output = maf.sample(100000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ayb3AGfHD1Mc"
   },
   "outputs": [],
   "source": [
    "def reconstruct(predictions):\n",
    "    \n",
    "    # split the output first\n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # recover scaler\n",
    "    pick_up_scaler = load(open('pick_up_scaler.pkl', 'rb'))\n",
    "    drop_off_scaler = load(open('drop_off_scaler.pkl', 'rb'))\n",
    "    pick_up_c = pick_up_scaler.inverse_transform(pick_up_c)\n",
    "    drop_off_c = drop_off_scaler.inverse_transform(drop_off_c)\n",
    "\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = pick_up_c[i][0]\n",
    "      y = pick_up_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      pick_up_c[i][0] = lon\n",
    "      pick_up_c[i][1] = lat\n",
    "    \n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = drop_off_c[i][0]\n",
    "      y = drop_off_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      drop_off_c[i][0] = lon\n",
    "      drop_off_c[i][1] = lat\n",
    "    \n",
    "    return np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Fca3K2cHZtD"
   },
   "outputs": [],
   "source": [
    "samples = reconstruct(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Sp-lrU6Hp49"
   },
   "outputs": [],
   "source": [
    "file_name = '100000_samples_MAF_12_100_100_batchnorm' + '.csv'\n",
    "np.savetxt(file_name, samples, delimiter = ',', header='pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, trip_duration' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyWIiBakI9AG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NYC_MAF_batchnorm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
