{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "-utLZuuaAbdU",
    "outputId": "bba778b4-4798-40ff-8048-9b51a7bf0b72"
   },
   "source": [
    "# Loading Model Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yfx72GctB69e"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqyH8qz8CmnK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t) / (1 - t))\n",
    "\n",
    "def meters2lonlat(x, y):\n",
    "    semimajoraxis = 6378137.0\n",
    "    lon = x / semimajoraxis / 0.017453292519943295\n",
    "    t = math.exp(y / 3189068.5)\n",
    "    lat = math.asin((t - 1) / (t + 1)) / 0.017453292519943295\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVK4YY7AnK2n"
   },
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../processed_nyc_train.csv',delimiter=',', skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "\n",
    "def format_data(dataset, pick_up_scaler=None, drop_off_scaler = None ,save_scaler=True):\n",
    "    \n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # Handling of the coordinates\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "        lon = pick_up_c[i][0]\n",
    "        lat = pick_up_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        pick_up_c[i][0] = x\n",
    "        pick_up_c[i][1] = y\n",
    "    \n",
    "    if pick_up_scaler is None:\n",
    "        pick_up_scaler = preprocessing.StandardScaler()\n",
    "        pick_up_scaler = pick_up_scaler.fit(pick_up_c)\n",
    "    \n",
    "    pick_up_c = pick_up_scaler.transform(pick_up_c)\n",
    "    \n",
    "    for i, c in enumerate(drop_off_c):\n",
    "        lon = drop_off_c[i][0]\n",
    "        lat = drop_off_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        drop_off_c[i][0] = x\n",
    "        drop_off_c[i][1] = y\n",
    "    \n",
    "    if drop_off_scaler is None:\n",
    "        drop_off_scaler = preprocessing.StandardScaler()\n",
    "        drop_off_scaler = drop_off_scaler.fit(drop_off_c)\n",
    "    \n",
    "    drop_off_c = drop_off_scaler.transform(drop_off_c)\n",
    "    \n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(pick_up_scaler, open('pick_up_scaler.pkl', 'wb'))\n",
    "        dump(drop_off_scaler, open('drop_off_scaler.pkl', 'wb'))\n",
    "\n",
    "    final = np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)\n",
    "    return final\n",
    "\n",
    "dataset = format_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dZ_ZqQ-niOWy",
    "outputId": "1f80263d-9449-466e-ff2a-977479ef3865"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.2.0-rc2\n",
      "tensorflow-probability:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization, ReLU, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print('tensorflow: ', tf.__version__)\n",
    "print('tensorflow-probability: ', tfp.__version__)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Case(Enum):\n",
    "    sampling = 1\n",
    "    density_estimation = 2\n",
    "\n",
    "class Made(tfk.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementation of a Masked Autoencoder for Distribution Estimation (MADE) [Germain et al. (2015)].\n",
    "    The existing TensorFlow bijector \"AutoregressiveNetwork\" is used. The output is reshaped to output one shift vector\n",
    "    and one log_scale vector.\n",
    "\n",
    "    :param params: Python integer specifying the number of parameters to output per input.\n",
    "    :param event_shape: Python list-like of positive integers (or a single int), specifying the shape of the input to this layer, which is also the event_shape of the distribution parameterized by this layer. Currently only rank-1 shapes are supported. That is, event_shape must be a single integer. If not specified, the event shape is inferred when this layer is first called or built.\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "    :param activation: An activation function. See tf.keras.layers.Dense. Default: None.\n",
    "    :param use_bias: Whether or not the dense layers constructed in this layer should have a bias term. See tf.keras.layers.Dense. Default: True.\n",
    "    :param kernel_regularizer: Regularizer function applied to the Dense kernel weight matrices. Default: None.\n",
    "    :param bias_regularizer: Regularizer function applied to the Dense bias weight vectors. Default: None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, event_shape=None, hidden_units=None, activation=None, use_bias=True,\n",
    "                 kernel_regularizer=None, bias_regularizer=None):\n",
    "\n",
    "        super(Made, self).__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.event_shape = event_shape\n",
    "        self.hidden_units = hidden_units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "\n",
    "        self.network = tfb.AutoregressiveNetwork(params=params, event_shape=event_shape, hidden_units=hidden_units,\n",
    "                                                 activation=activation, use_bias=use_bias, kernel_regularizer=kernel_regularizer, \n",
    "                                                 bias_regularizer=bias_regularizer)\n",
    "\n",
    "    def call(self, x):\n",
    "        shift, log_scale = tf.unstack(self.network(x), num=2, axis=-1)\n",
    "\n",
    "        return shift, tf.math.tanh(log_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "s0EpDb2rixJS",
    "outputId": "29583f52-4aa9-4ff7-a244-d31e6ad959aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/bijectors/masked_autoregressive.py:310: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n"
     ]
    }
   ],
   "source": [
    "hidden_shape = [100, 100]  # hidden shape for MADE network of MAF\n",
    "layers = 12  # number of layers of the flow\n",
    "event_shape=[6]\n",
    "\n",
    "base_dist = tfd.Normal(loc=0.0, scale=1.0)  # specify base distribution\n",
    "\n",
    "bijectors = []\n",
    "for i in range(0, layers):\n",
    "    bijectors.append(tfb.MaskedAutoregressiveFlow(shift_and_log_scale_fn = Made(params=2, event_shape=event_shape, hidden_units=hidden_shape, activation=\"relu\")))\n",
    "    bijectors.append(tfb.Permute(permutation=[5,4,3,2,1,0]))  # data permutation after layers of MAF\n",
    "    \n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_maf')\n",
    "\n",
    "maf = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector,\n",
    "    event_shape=event_shape\n",
    ")\n",
    "\n",
    "# initialize flow\n",
    "samples = maf.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTG7WFpSmmQL"
   },
   "outputs": [],
   "source": [
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(5e3)  # maximum number of epochs of the training\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAfkJRh-mnGQ"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
    "\n",
    "x_ = tf.keras.layers.Input(shape=event_shape, dtype=tf.float32)\n",
    "log_prob_ = maf.log_prob(x_)\n",
    "model = tf.keras.Model(x_, log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqZHDr1Loa-F"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt,\n",
    "              loss=lambda _, log_prob: -log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHLlenbxlTqS"
   },
   "source": [
    "# Setting Up Checkpoints ( Will Save Model After Every Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SF9uXEt6oo6d"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "weight_file = './checkpoint/cp.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "CWW-n9mZpxVV",
    "outputId": "4cb747e1-b70a-4d06-d27d-8584fe70da7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14586/14586 [==============================] - 327s 22ms/step - loss: 257528.6094\n",
      "14586/14586 [==============================] - 323s 22ms/step - loss: 574.2960\n",
      "14586/14586 [==============================] - 325s 22ms/step - loss: 128.5223\n",
      "14586/14586 [==============================] - 321s 22ms/step - loss: 664.0750\n",
      "14586/14586 [==============================] - 318s 22ms/step - loss: 26.6973\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 21.7154\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 283.4973\n",
      "14586/14586 [==============================] - 320s 22ms/step - loss: 29.2217\n",
      "14586/14586 [==============================] - 317s 22ms/step - loss: 34.1872\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 46.4971\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 42.3621\n",
      "14586/14586 [==============================] - 318s 22ms/step - loss: 30.0322\n",
      "14586/14586 [==============================] - 320s 22ms/step - loss: 33.2788\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 22.2252\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 27.6141\n",
      "14586/14586 [==============================] - 318s 22ms/step - loss: 112.4017\n",
      "14586/14586 [==============================] - 318s 22ms/step - loss: 84.4263\n",
      "14586/14586 [==============================] - 318s 22ms/step - loss: 66.3945\n",
      "14586/14586 [==============================] - 320s 22ms/step - loss: 41.7611\n",
      "14586/14586 [==============================] - 312s 21ms/step - loss: 23.6498\n",
      "14586/14586 [==============================] - 316s 22ms/step - loss: 36.0379\n",
      "14586/14586 [==============================] - 323s 22ms/step - loss: 4905.0063\n",
      "14586/14586 [==============================] - 321s 22ms/step - loss: 293.1844\n",
      "14586/14586 [==============================] - 320s 22ms/step - loss: 107.9275\n",
      "14586/14586 [==============================] - 323s 22ms/step - loss: 42.2030\n",
      "14586/14586 [==============================] - 322s 22ms/step - loss: 69.6323\n",
      "14586/14586 [==============================] - 321s 22ms/step - loss: 31.5871\n",
      "14586/14586 [==============================] - 324s 22ms/step - loss: 39.9439\n",
      "14586/14586 [==============================] - 323s 22ms/step - loss: 25.2726\n",
      "14586/14586 [==============================] - 321s 22ms/step - loss: 447.5041\n",
      "14586/14586 [==============================] - 321s 22ms/step - loss: 43.6815\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 26.8501\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 21.5903\n",
      "14586/14586 [==============================] - 323s 22ms/step - loss: 39.4965\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 16.5215\n",
      "14586/14586 [==============================] - 319s 22ms/step - loss: 83.1785\n",
      "14586/14586 [==============================] - 316s 22ms/step - loss: 33.0905\n",
      "14586/14586 [==============================] - 314s 22ms/step - loss: 65.5658\n",
      "14586/14586 [==============================] - 316s 22ms/step - loss: 37.9195\n",
      "14586/14586 [==============================] - 321s 22ms/step - loss: 83.6348\n",
      "14586/14586 [==============================] - 316s 22ms/step - loss: 41.2409\n",
      " 7473/14586 [==============>...............] - ETA: 2:33 - loss: 39.6500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-da2264f0fa3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             verbose=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(50): # change number of epochs accordingly\n",
    "  model.load_weights(weight_file)\n",
    "  model.fit(x=dataset,\n",
    "            y=np.zeros((dataset.shape[0], 0), dtype=np.float32),\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=dataset.shape[0] // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=True)\n",
    "  model.save_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWLPuKFuuVDz"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"MAF_12_100_100.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gmcYetC7Ds-o"
   },
   "outputs": [],
   "source": [
    "output = maf.sample(100000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ayb3AGfHD1Mc"
   },
   "outputs": [],
   "source": [
    "def reconstruct(predictions):\n",
    "    \n",
    "    # split the output first\n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # recover scaler\n",
    "    pick_up_scaler = load(open('pick_up_scaler.pkl', 'rb'))\n",
    "    drop_off_scaler = load(open('drop_off_scaler.pkl', 'rb'))\n",
    "    pick_up_c = pick_up_scaler.inverse_transform(pick_up_c)\n",
    "    drop_off_c = drop_off_scaler.inverse_transform(drop_off_c)\n",
    "\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = pick_up_c[i][0]\n",
    "      y = pick_up_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      pick_up_c[i][0] = lon\n",
    "      pick_up_c[i][1] = lat\n",
    "    \n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = drop_off_c[i][0]\n",
    "      y = drop_off_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      drop_off_c[i][0] = lon\n",
    "      drop_off_c[i][1] = lat\n",
    "    \n",
    "    return np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Fca3K2cHZtD"
   },
   "outputs": [],
   "source": [
    "samples = reconstruct(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Sp-lrU6Hp49"
   },
   "outputs": [],
   "source": [
    "file_name = '100000_samples_MAF_12_100_100' + '.csv'\n",
    "np.savetxt(file_name, samples, delimiter = ',', header='pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, trip_duration' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WyWIiBakI9AG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NYC dataset test MAF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
