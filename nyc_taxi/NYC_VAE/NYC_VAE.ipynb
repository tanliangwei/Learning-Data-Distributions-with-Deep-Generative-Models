{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRxH9JrNNzpl"
   },
   "source": [
    "# Model Training\n",
    "In this notebook, we will train our VAE model. This involves:\n",
    "\n",
    "1. Encoder\n",
    "2. Decoder\n",
    "3. Full Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDk__FJlNzpy"
   },
   "source": [
    "## Importing Packages\n",
    "We will be using Keras to build and train our VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlgGkaseNzqB"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import h5py\n",
    "\n",
    "# import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Layer, Add, Multiply, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import mdn\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RKRB3NMWNzqo"
   },
   "source": [
    "## Definition of Constants\n",
    "Hyper parameters and constants will all be located here for ease of adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5Fl4AYsNzqq"
   },
   "outputs": [],
   "source": [
    "continuous_dim = 6\n",
    "original_dim = 6\n",
    "intermediate_dim_1 = 100\n",
    "intermediate_dim_2 = 100\n",
    "latent_dim = 4\n",
    "batch_size = 100\n",
    "epochs = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "## Constants for the Mixture layer\n",
    "N_HIDDEN = 15  # number of hidden units in the Dense layer\n",
    "N_MIXES = 10  # number of mixture components\n",
    "OUTPUT_DIMS = 2  # number of real-values predicted by each mixture component\n",
    "\n",
    "# Model File Paths\n",
    "full_model_file = \"vae_full_model.json\"\n",
    "full_model_weights = \"vae_full_model.h5\"\n",
    "encoder_file = \"vae_encoder.json\"\n",
    "encoder_weights = \"vae_encoder.h5\"\n",
    "decoder_file = 'vae_decoder.json'\n",
    "decoder_weights = 'vae_decoder.h5'\n",
    "z_meta_file = 'z_meta.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jtIz99MoNzqu"
   },
   "source": [
    "## Building the Model\n",
    "We have to write our own custom layer and custom loss function as these are not supported on Keras natively. There are a few things to be done:\n",
    "\n",
    "1. Custom KLDivergence Layer\n",
    "2. Custom Loss Functions\n",
    "3. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5Z7IiUwNzqv"
   },
   "source": [
    "### KL Divergence Layer\n",
    "To ensure modularity, we decided to create a separate layer for KL Divergence. This layer will account for the loss required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDOWHUdPNzqw"
   },
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CO-HTbE1Nzrf"
   },
   "source": [
    "### Custom Loss Functions\n",
    "As the yelp dataset contains binary, categorical as well as continuous data, we will build 3 custom loss functions.\n",
    "\n",
    "#### Binary Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fjGhgKQNzrh"
   },
   "outputs": [],
   "source": [
    "def binary_loss(y_true, y_pred):\n",
    "\t# input dimension is (batchsize, 1)\n",
    "    return K.binary_crossentropy(y_true, y_pred) # the dimension of return value is (batchsize , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdaBeX4LNzrx"
   },
   "source": [
    "#### Categorical Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuW_FqUtNzrz"
   },
   "outputs": [],
   "source": [
    "def categorical_loss(y_true, y_pred):\n",
    "\t# input dimension is (batchsize, number of categories)\n",
    "  return K.categorical_crossentropy(y_true, y_pred) # the dimension of return value is (batchsize , 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp3vbrgcNzsl"
   },
   "source": [
    "#### Continuous Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d49IVJlCNzsr"
   },
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis) # return a tensor of shape (batch_size, 1)\n",
    "\n",
    "# Added a postfix because now we also have poisson as a distribution\n",
    "def continuous_loss_gaussian(y_true, y_pred):\n",
    "\t# need to return log probability for continuous gaussian loss.\n",
    "\t# will get a (batchsize, 6 continuous variable input) where 3 of the 6 represents mu and the others logvar\n",
    "\t# y_true will be (batchsize, 3)\n",
    "  mu, logvar = tf.split(y_pred, num_or_size_splits = 2, axis = 1)\n",
    "  return -1 * log_normal_pdf(y_true, mu, logvar) \n",
    "\n",
    "def continuous_loss_poisson(y_true, y_pred):\n",
    "\t# need to return log probability for continuous poisson loss.\n",
    "\t# will get a (batchsize, 6 continuous variable input) where 3 of the 6 represents mu and the others logvar\n",
    "\t# y_true will be (batchsize, 3)\n",
    "  return tf.nn.log_poisson_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsSoL9t-Nzsx"
   },
   "source": [
    "### Model Architecture\n",
    "- tanh/sigmoid is used because Relu resulted in loss going to infinity\n",
    "- going to delete review_log_var and review_mu since we are trying out the Poission distribution which only takes 1 parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJPe7vm5Nzsz"
   },
   "outputs": [],
   "source": [
    "# x = Input(shape=(original_dim,), name='input_x')\n",
    "# h = Dense(intermediate_dim_1, activation='tanh', name='hidden_enc_1')(x)\n",
    "# h = Dense(intermediate_dim_2, activation='tanh', name='hidden_enc_2')(h)\n",
    "\n",
    "# z_mu = Dense(latent_dim, name='z_mu')(h)\n",
    "# z_log_var = Dense(latent_dim, name='z_log_var')(h)\n",
    "\n",
    "# z_mu, z_log_var = KLDivergenceLayer(name='KL_Divergence')([z_mu, z_log_var])\n",
    "# z_sigma = Lambda(lambda t: K.exp(.5*t))(z_log_var)\n",
    "\n",
    "# eps = Input(name='input_eps',tensor=K.random_normal(stddev=epsilon_std,\n",
    "#                                    shape=(K.shape(x)[0], latent_dim)))\n",
    "# z_eps = Multiply()([z_sigma, eps])\n",
    "# z = Add()([z_mu, z_eps])\n",
    "\n",
    "# decode_1 = Dense(intermediate_dim_2, activation='tanh', name='hidden_dec_2')\n",
    "# h_dec = decode_1(z)\n",
    "\n",
    "# decode_2 = Dense(intermediate_dim_1, activation='tanh', name='hidden_dec_1')\n",
    "# h_dec = decode_2(h_dec)\n",
    "\n",
    "\n",
    "# ## This outputs the lambda require for poisson distribution and thats all that we need\n",
    "# x_pred_passenger_count_log_lambda_layer = Dense(1,name = 'x_pred_passenger_count_log_lambda_layer')\n",
    "# x_pred_passenger_count = x_pred_passenger_count_log_lambda_layer(h_dec)\n",
    "\n",
    "# ## This outputs the lambda require for poisson distribution and thats all that we need\n",
    "# x_pred_travel_duration_log_lambda_layer = Dense(1,name = 'x_pred_travel_duration_log_lambda_layer')\n",
    "# x_pred_travel_duration = x_pred_travel_duration_log_lambda_layer(h_dec)\n",
    "\n",
    "\n",
    "# vae = Model(inputs=[x,eps], outputs=[x_pred_coordinates,x_pred_coordinates_2, x_pred_passenger_count, x_pred_travel_duration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yrRCp8WmQ0-s",
    "outputId": "0f9da8f8-355c-45c0-eba7-87afd5f05a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"hidden_dec_2/Identity:0\", shape=(None, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfpl = tfp.layers\n",
    "tfd = tfp.distributions\n",
    "\n",
    "\n",
    "input_shape = (original_dim,)\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(latent_dim), scale=1), reinterpreted_batch_ndims=1)\n",
    "\n",
    "## Encoder\n",
    "encoder = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=input_shape, name = 'input_x'),\n",
    "    tfkl.Dense(intermediate_dim_1, activation='tanh', name='hidden_enc_1'),\n",
    "    tfkl.Dense(intermediate_dim_2, activation='tanh', name='hidden_enc_2'),\n",
    "    tfkl.Dense(tfpl.IndependentNormal.params_size(latent_dim), activation=None, name = 'z_probability_distribution'),\n",
    "])\n",
    "\n",
    "sampler = tfk.Sequential([\n",
    "    tfkl.InputLayer(input_shape=(tfpl.IndependentNormal.params_size(latent_dim), ), name = 'input_z_params'),\n",
    "    tfpl.IndependentNormal( latent_dim, name = 'sample_layer'),\n",
    "    tfpl.KLDivergenceAddLoss(prior),\n",
    "])\n",
    "decode_1 = tfkl.Dense(intermediate_dim_2, activation='tanh', name='hidden_dec_2')\n",
    "h_dec = decode_1(sampler(encoder.outputs[0]))\n",
    "print(h_dec)\n",
    "\n",
    "decode_2 = tfkl.Dense(intermediate_dim_1, activation='tanh', name='hidden_dec_1')\n",
    "h_dec = decode_2(h_dec)\n",
    "\n",
    "\n",
    "x_pred_coordinates_mu_layer = Dense(2, name='x_pred_coordinates_mu')\n",
    "x_pred_coordinates_mu = x_pred_coordinates_mu_layer(h_dec)\n",
    "\n",
    "x_pred_coordinates_log_var_layer = Dense(2, name='x_pred_coordinates_log_var')\n",
    "x_pred_coordinates_log_var = x_pred_coordinates_log_var_layer(h_dec)\n",
    "\n",
    "x_pred_coordinates_layer = Concatenate(axis=-1, name = 'x_pred_coordinates')\n",
    "x_pred_coordinates = x_pred_coordinates_layer([x_pred_coordinates_mu, x_pred_coordinates_log_var])\n",
    "\n",
    "x_pred_coordinates_2_mu_layer = Dense(2, name='x_pred_coordinates_2_mu')\n",
    "x_pred_coordinates_2_mu = x_pred_coordinates_2_mu_layer(h_dec)\n",
    "\n",
    "x_pred_coordinates_2_log_var_layer = Dense(2, name='x_pred_coordinates_2_log_var')\n",
    "x_pred_coordinates_2_log_var = x_pred_coordinates_2_log_var_layer(h_dec)\n",
    "\n",
    "x_pred_coordinates_2_layer = Concatenate(axis=-1, name = 'x_pred_coordinates_2')\n",
    "x_pred_coordinates_2 = x_pred_coordinates_2_layer([x_pred_coordinates_2_mu, x_pred_coordinates_2_log_var])\n",
    "\n",
    "\n",
    "## This outputs the lambda require for poisson distribution and thats all that we need\n",
    "x_pred_travel_duration_log_lambda_layer = tfkl.Dense(1,name = 'x_pred_travel_duration_log_lambda_layer')\n",
    "x_pred_travel_duration = x_pred_travel_duration_log_lambda_layer(h_dec)\n",
    "\n",
    "\n",
    "x_pred_passenger_count_log_lambda_layer = tfkl.Dense(1,name = 'x_pred_passenger_count_log_lambda_layer')\n",
    "x_pred_passenger_count = x_pred_passenger_count_log_lambda_layer(h_dec)\n",
    "\n",
    "vae = tfk.Model(inputs=encoder.inputs, outputs=[x_pred_coordinates,x_pred_coordinates_2, x_pred_passenger_count, x_pred_travel_duration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G57JiygKNztr"
   },
   "source": [
    "### Compiling Model and Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VSzajCacNzts"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "vae.compile(optimizer=optimizer, loss=[continuous_loss_gaussian, continuous_loss_gaussian, continuous_loss_poisson, continuous_loss_poisson], loss_weights=[1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "qqYAn85wNztw",
    "outputId": "2e737cb6-165d-4875-8ab7-1ef7695faa28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_x (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "hidden_enc_1 (Dense)            (None, 100)          700         input_x[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "hidden_enc_2 (Dense)            (None, 100)          10100       hidden_enc_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "z_probability_distribution (Den (None, 8)            808         hidden_enc_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 4)            0           z_probability_distribution[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "hidden_dec_2 (Dense)            (None, 100)          500         sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "hidden_dec_1 (Dense)            (None, 100)          10100       hidden_dec_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates_mu (Dense)   (None, 2)            202         hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates_log_var (Den (None, 2)            202         hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates_2_mu (Dense) (None, 2)            202         hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates_2_log_var (D (None, 2)            202         hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates (Concatenate (None, 4)            0           x_pred_coordinates_mu[0][0]      \n",
      "                                                                 x_pred_coordinates_log_var[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_coordinates_2 (Concatena (None, 4)            0           x_pred_coordinates_2_mu[0][0]    \n",
      "                                                                 x_pred_coordinates_2_log_var[0][0\n",
      "__________________________________________________________________________________________________\n",
      "x_pred_passenger_count_log_lamb (None, 1)            101         hidden_dec_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "x_pred_travel_duration_log_lamb (None, 1)            101         hidden_dec_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 23,218\n",
      "Trainable params: 23,218\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Yz2CNJVNzt-"
   },
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjWJR8w8QG_D"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t) / (1 - t))\n",
    "\n",
    "def meters2lonlat(x, y):\n",
    "    semimajoraxis = 6378137.0\n",
    "    lon = x / semimajoraxis / 0.017453292519943295\n",
    "    t = math.exp(y / 3189068.5)\n",
    "    lat = math.asin((t - 1) / (t + 1)) / 0.017453292519943295\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_wFDY6HGNzt_"
   },
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../processed_nyc_train.csv',delimiter=',', skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "\n",
    "def format_data(dataset, pick_up_scaler=None, drop_off_scaler = None ,save_scaler=True):\n",
    "    \n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # Handling of the coordinates\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "        lon = pick_up_c[i][0]\n",
    "        lat = pick_up_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        pick_up_c[i][0] = x\n",
    "        pick_up_c[i][1] = y\n",
    "    \n",
    "    if pick_up_scaler is None:\n",
    "        pick_up_scaler = preprocessing.StandardScaler()\n",
    "        pick_up_scaler = pick_up_scaler.fit(pick_up_c)\n",
    "    \n",
    "    pick_up_c = pick_up_scaler.transform(pick_up_c)\n",
    "    \n",
    "    for i, c in enumerate(drop_off_c):\n",
    "        lon = drop_off_c[i][0]\n",
    "        lat = drop_off_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        drop_off_c[i][0] = x\n",
    "        drop_off_c[i][1] = y\n",
    "    \n",
    "    if drop_off_scaler is None:\n",
    "        drop_off_scaler = preprocessing.StandardScaler()\n",
    "        drop_off_scaler = drop_off_scaler.fit(drop_off_c)\n",
    "    \n",
    "    drop_off_c = drop_off_scaler.transform(drop_off_c)\n",
    "    \n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(pick_up_scaler, open('pick_up_scaler.pkl', 'wb'))\n",
    "        dump(drop_off_scaler, open('drop_off_scaler.pkl', 'wb'))\n",
    "\n",
    "    final = np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)\n",
    "    return final, pick_up_c, drop_off_c, num_passenger, travel_duration\n",
    "\n",
    "dataset, pick_up_c, drop_off_c, num_passenger, travel_duration = format_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fmz64xFWNzuG"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NbyuszvRNzuH",
    "outputId": "b3ee9eb8-8248-4cb4-ab07-9c8831193508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -5948.7061 - x_pred_coordinates_loss: 2.1724 - x_pred_coordinates_2_loss: 2.9726 - x_pred_passenger_count_log_lambda_layer_loss: 0.8069 - x_pred_travel_duration_log_lambda_layer_loss: -5967.7407\n",
      "epoch  1\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -5995.0723 - x_pred_coordinates_loss: 2.1392 - x_pred_coordinates_2_loss: 2.3784 - x_pred_passenger_count_log_lambda_layer_loss: 0.7776 - x_pred_travel_duration_log_lambda_layer_loss: -6015.4336\n",
      "epoch  2\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6037.5283 - x_pred_coordinates_loss: 2.1547 - x_pred_coordinates_2_loss: 2.1389 - x_pred_passenger_count_log_lambda_layer_loss: 0.7612 - x_pred_travel_duration_log_lambda_layer_loss: -6058.6934\n",
      "epoch  3\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -5924.7964 - x_pred_coordinates_loss: 2.1239 - x_pred_coordinates_2_loss: 2.1983 - x_pred_passenger_count_log_lambda_layer_loss: 0.7737 - x_pred_travel_duration_log_lambda_layer_loss: -5945.4741\n",
      "epoch  4\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -5992.7881 - x_pred_coordinates_loss: 2.1136 - x_pred_coordinates_2_loss: 2.1216 - x_pred_passenger_count_log_lambda_layer_loss: 0.7460 - x_pred_travel_duration_log_lambda_layer_loss: -6014.3164\n",
      "epoch  5\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6039.0015 - x_pred_coordinates_loss: 2.4051 - x_pred_coordinates_2_loss: 2.3615 - x_pred_passenger_count_log_lambda_layer_loss: 0.7081 - x_pred_travel_duration_log_lambda_layer_loss: -6061.5513\n",
      "epoch  6\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -5966.1670 - x_pred_coordinates_loss: 2.7275 - x_pred_coordinates_2_loss: 2.9131 - x_pred_passenger_count_log_lambda_layer_loss: 0.7662 - x_pred_travel_duration_log_lambda_layer_loss: -5991.2290\n",
      "epoch  7\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6081.8799 - x_pred_coordinates_loss: 2.6464 - x_pred_coordinates_2_loss: 2.6276 - x_pred_passenger_count_log_lambda_layer_loss: 0.7976 - x_pred_travel_duration_log_lambda_layer_loss: -6103.6387\n",
      "epoch  8\n",
      "14587/14587 [==============================] - 70s 5ms/step - loss: -6043.7134 - x_pred_coordinates_loss: 2.6034 - x_pred_coordinates_2_loss: 2.6564 - x_pred_passenger_count_log_lambda_layer_loss: 0.8071 - x_pred_travel_duration_log_lambda_layer_loss: -6069.6499\n",
      "epoch  9\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -5985.7661 - x_pred_coordinates_loss: 2.4147 - x_pred_coordinates_2_loss: 2.4968 - x_pred_passenger_count_log_lambda_layer_loss: 0.7969 - x_pred_travel_duration_log_lambda_layer_loss: -6015.9150\n",
      "epoch  10\n",
      "14587/14587 [==============================] - 76s 5ms/step - loss: -5962.5171 - x_pred_coordinates_loss: 2.2700 - x_pred_coordinates_2_loss: 2.3507 - x_pred_passenger_count_log_lambda_layer_loss: 0.7618 - x_pred_travel_duration_log_lambda_layer_loss: -5990.0200\n",
      "epoch  11\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -5964.3398 - x_pred_coordinates_loss: 2.0699 - x_pred_coordinates_2_loss: 2.1439 - x_pred_passenger_count_log_lambda_layer_loss: 0.7838 - x_pred_travel_duration_log_lambda_layer_loss: -5991.4468\n",
      "epoch  12\n",
      "14587/14587 [==============================] - 74s 5ms/step - loss: -5965.5332 - x_pred_coordinates_loss: 2.1204 - x_pred_coordinates_2_loss: 2.3221 - x_pred_passenger_count_log_lambda_layer_loss: 0.8048 - x_pred_travel_duration_log_lambda_layer_loss: -5994.2998\n",
      "epoch  13\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -5970.4365 - x_pred_coordinates_loss: 2.0681 - x_pred_coordinates_2_loss: 2.1724 - x_pred_passenger_count_log_lambda_layer_loss: 0.8119 - x_pred_travel_duration_log_lambda_layer_loss: -6000.9922\n",
      "epoch  14\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6024.4644 - x_pred_coordinates_loss: 2.1410 - x_pred_coordinates_2_loss: 2.2400 - x_pred_passenger_count_log_lambda_layer_loss: 0.8021 - x_pred_travel_duration_log_lambda_layer_loss: -6053.1611\n",
      "epoch  15\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -5999.9194 - x_pred_coordinates_loss: 2.0514 - x_pred_coordinates_2_loss: 2.1261 - x_pred_passenger_count_log_lambda_layer_loss: 0.8098 - x_pred_travel_duration_log_lambda_layer_loss: -6028.4238\n",
      "epoch  16\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -6068.2959 - x_pred_coordinates_loss: 2.1165 - x_pred_coordinates_2_loss: 2.1482 - x_pred_passenger_count_log_lambda_layer_loss: 0.8142 - x_pred_travel_duration_log_lambda_layer_loss: -6095.5630\n",
      "epoch  17\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6062.0220 - x_pred_coordinates_loss: 2.0505 - x_pred_coordinates_2_loss: 2.0666 - x_pred_passenger_count_log_lambda_layer_loss: 0.7952 - x_pred_travel_duration_log_lambda_layer_loss: -6088.7246\n",
      "epoch  18\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6041.1870 - x_pred_coordinates_loss: 3.0341 - x_pred_coordinates_2_loss: 3.0469 - x_pred_passenger_count_log_lambda_layer_loss: 0.8005 - x_pred_travel_duration_log_lambda_layer_loss: -6070.5298\n",
      "epoch  19\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6104.6465 - x_pred_coordinates_loss: 2.6387 - x_pred_coordinates_2_loss: 2.5850 - x_pred_passenger_count_log_lambda_layer_loss: 0.8120 - x_pred_travel_duration_log_lambda_layer_loss: -6130.4517\n",
      "epoch  20\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -6064.7935 - x_pred_coordinates_loss: 2.0545 - x_pred_coordinates_2_loss: 2.0784 - x_pred_passenger_count_log_lambda_layer_loss: 0.8034 - x_pred_travel_duration_log_lambda_layer_loss: -6090.7114\n",
      "epoch  21\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6008.6992 - x_pred_coordinates_loss: 1.9746 - x_pred_coordinates_2_loss: 2.0177 - x_pred_passenger_count_log_lambda_layer_loss: 0.8103 - x_pred_travel_duration_log_lambda_layer_loss: -6033.4678\n",
      "epoch  22\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6067.7119 - x_pred_coordinates_loss: 1.9674 - x_pred_coordinates_2_loss: 2.0223 - x_pred_passenger_count_log_lambda_layer_loss: 0.7736 - x_pred_travel_duration_log_lambda_layer_loss: -6091.3579\n",
      "epoch  23\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6109.3242 - x_pred_coordinates_loss: 1.9605 - x_pred_coordinates_2_loss: 2.0226 - x_pred_passenger_count_log_lambda_layer_loss: 0.8082 - x_pred_travel_duration_log_lambda_layer_loss: -6132.4790\n",
      "epoch  24\n",
      "14587/14587 [==============================] - 74s 5ms/step - loss: -6020.9546 - x_pred_coordinates_loss: 1.9387 - x_pred_coordinates_2_loss: 1.9759 - x_pred_passenger_count_log_lambda_layer_loss: 0.7966 - x_pred_travel_duration_log_lambda_layer_loss: -6043.3730\n",
      "epoch  25\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -6035.7388 - x_pred_coordinates_loss: 2.0044 - x_pred_coordinates_2_loss: 2.0684 - x_pred_passenger_count_log_lambda_layer_loss: 0.8074 - x_pred_travel_duration_log_lambda_layer_loss: -6058.8760\n",
      "epoch  26\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -5992.6436 - x_pred_coordinates_loss: 2.0752 - x_pred_coordinates_2_loss: 2.0833 - x_pred_passenger_count_log_lambda_layer_loss: 0.7694 - x_pred_travel_duration_log_lambda_layer_loss: -6017.2153\n",
      "epoch  27\n",
      "14587/14587 [==============================] - 74s 5ms/step - loss: -6078.6304 - x_pred_coordinates_loss: 1.9174 - x_pred_coordinates_2_loss: 1.9723 - x_pred_passenger_count_log_lambda_layer_loss: 0.8156 - x_pred_travel_duration_log_lambda_layer_loss: -6102.2388\n",
      "epoch  28\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -6062.3457 - x_pred_coordinates_loss: 1.9281 - x_pred_coordinates_2_loss: 1.9831 - x_pred_passenger_count_log_lambda_layer_loss: 0.8075 - x_pred_travel_duration_log_lambda_layer_loss: -6085.1343\n",
      "epoch  29\n",
      "14587/14587 [==============================] - 73s 5ms/step - loss: -6055.7603 - x_pred_coordinates_loss: 1.9779 - x_pred_coordinates_2_loss: 2.0277 - x_pred_passenger_count_log_lambda_layer_loss: 0.7975 - x_pred_travel_duration_log_lambda_layer_loss: -6078.7495\n",
      "epoch  30\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6090.4873 - x_pred_coordinates_loss: 1.8869 - x_pred_coordinates_2_loss: 1.9846 - x_pred_passenger_count_log_lambda_layer_loss: 0.8135 - x_pred_travel_duration_log_lambda_layer_loss: -6113.2671\n",
      "epoch  31\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6089.7598 - x_pred_coordinates_loss: 1.9342 - x_pred_coordinates_2_loss: 2.0018 - x_pred_passenger_count_log_lambda_layer_loss: 0.8086 - x_pred_travel_duration_log_lambda_layer_loss: -6111.8975\n",
      "epoch  32\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6051.2930 - x_pred_coordinates_loss: 1.9182 - x_pred_coordinates_2_loss: 1.9849 - x_pred_passenger_count_log_lambda_layer_loss: 0.8138 - x_pred_travel_duration_log_lambda_layer_loss: -6073.0425\n",
      "epoch  33\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6094.4424 - x_pred_coordinates_loss: 1.9894 - x_pred_coordinates_2_loss: 2.0516 - x_pred_passenger_count_log_lambda_layer_loss: 0.7884 - x_pred_travel_duration_log_lambda_layer_loss: -6116.0723\n",
      "epoch  34\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6131.2578 - x_pred_coordinates_loss: 1.9218 - x_pred_coordinates_2_loss: 1.9801 - x_pred_passenger_count_log_lambda_layer_loss: 0.8066 - x_pred_travel_duration_log_lambda_layer_loss: -6152.6895\n",
      "epoch  35\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6068.2471 - x_pred_coordinates_loss: 1.8789 - x_pred_coordinates_2_loss: 1.9420 - x_pred_passenger_count_log_lambda_layer_loss: 0.8025 - x_pred_travel_duration_log_lambda_layer_loss: -6089.7080\n",
      "epoch  36\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6111.4536 - x_pred_coordinates_loss: 2.4947 - x_pred_coordinates_2_loss: 2.4507 - x_pred_passenger_count_log_lambda_layer_loss: 0.8055 - x_pred_travel_duration_log_lambda_layer_loss: -6133.8896\n",
      "epoch  37\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6117.3511 - x_pred_coordinates_loss: 1.8801 - x_pred_coordinates_2_loss: 1.9571 - x_pred_passenger_count_log_lambda_layer_loss: 0.8143 - x_pred_travel_duration_log_lambda_layer_loss: -6138.3911\n",
      "epoch  38\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6107.3350 - x_pred_coordinates_loss: 1.9600 - x_pred_coordinates_2_loss: 1.9973 - x_pred_passenger_count_log_lambda_layer_loss: 0.8050 - x_pred_travel_duration_log_lambda_layer_loss: -6127.8706\n",
      "epoch  39\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6079.3022 - x_pred_coordinates_loss: 2.0318 - x_pred_coordinates_2_loss: 2.0856 - x_pred_passenger_count_log_lambda_layer_loss: 0.7996 - x_pred_travel_duration_log_lambda_layer_loss: -6099.2437\n",
      "epoch  40\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6072.7241 - x_pred_coordinates_loss: 2.0016 - x_pred_coordinates_2_loss: 2.0559 - x_pred_passenger_count_log_lambda_layer_loss: 0.8170 - x_pred_travel_duration_log_lambda_layer_loss: -6093.2251\n",
      "epoch  41\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6046.3647 - x_pred_coordinates_loss: 1.9189 - x_pred_coordinates_2_loss: 1.9939 - x_pred_passenger_count_log_lambda_layer_loss: 0.8096 - x_pred_travel_duration_log_lambda_layer_loss: -6066.1553\n",
      "epoch  42\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6122.5298 - x_pred_coordinates_loss: 1.8913 - x_pred_coordinates_2_loss: 1.9734 - x_pred_passenger_count_log_lambda_layer_loss: 0.7949 - x_pred_travel_duration_log_lambda_layer_loss: -6142.2944\n",
      "epoch  43\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6127.6499 - x_pred_coordinates_loss: 2.0223 - x_pred_coordinates_2_loss: 2.0730 - x_pred_passenger_count_log_lambda_layer_loss: 0.7662 - x_pred_travel_duration_log_lambda_layer_loss: -6146.5254\n",
      "epoch  44\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6117.1782 - x_pred_coordinates_loss: 2.0450 - x_pred_coordinates_2_loss: 2.1020 - x_pred_passenger_count_log_lambda_layer_loss: 0.8166 - x_pred_travel_duration_log_lambda_layer_loss: -6137.5591\n",
      "epoch  45\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6115.3735 - x_pred_coordinates_loss: 1.9906 - x_pred_coordinates_2_loss: 2.0355 - x_pred_passenger_count_log_lambda_layer_loss: 0.8115 - x_pred_travel_duration_log_lambda_layer_loss: -6135.2954\n",
      "epoch  46\n",
      "14587/14587 [==============================] - 72s 5ms/step - loss: -6074.9595 - x_pred_coordinates_loss: 1.9172 - x_pred_coordinates_2_loss: 2.0008 - x_pred_passenger_count_log_lambda_layer_loss: 0.8246 - x_pred_travel_duration_log_lambda_layer_loss: -6093.9556\n",
      "epoch  47\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6102.7944 - x_pred_coordinates_loss: 1.9179 - x_pred_coordinates_2_loss: 1.9404 - x_pred_passenger_count_log_lambda_layer_loss: 0.7902 - x_pred_travel_duration_log_lambda_layer_loss: -6120.7295\n",
      "epoch  48\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6116.8320 - x_pred_coordinates_loss: 1.8751 - x_pred_coordinates_2_loss: 1.9784 - x_pred_passenger_count_log_lambda_layer_loss: 0.8070 - x_pred_travel_duration_log_lambda_layer_loss: -6134.8218\n",
      "epoch  49\n",
      "14587/14587 [==============================] - 71s 5ms/step - loss: -6134.2808 - x_pred_coordinates_loss: 2.0327 - x_pred_coordinates_2_loss: 2.0351 - x_pred_passenger_count_log_lambda_layer_loss: 0.8077 - x_pred_travel_duration_log_lambda_layer_loss: -6151.5547\n"
     ]
    }
   ],
   "source": [
    "for i in range(50): # change to desired number of epochs\n",
    "  print('epoch ' ,i)\n",
    "  vae.load_weights('weights_2.h5')\n",
    "  vae.fit(dataset , [pick_up_c, drop_off_c, num_passenger, travel_duration], shuffle = True, epochs = 1, batch_size = batch_size)\n",
    "  vae.save_weights('weights_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TapZxoJJ7ZnP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCkrywvPNzuP"
   },
   "source": [
    "## Building the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6ovgEYeNzuQ"
   },
   "outputs": [],
   "source": [
    "decode_input = Input(shape=(latent_dim, ), name = 'decode_input')\n",
    "decode_layer_1 = decode_1(decode_input)\n",
    "decode_layer_2 = decode_2(decode_layer_1)\n",
    "\n",
    "x_pred_coordinates_mu = x_pred_coordinates_mu_layer(decode_layer_2)\n",
    "x_pred_coordinates_log_var = x_pred_coordinates_log_var_layer(decode_layer_2)\n",
    "decode_x_pred_coordinates = x_pred_coordinates_layer([x_pred_coordinates_mu, x_pred_coordinates_log_var])\n",
    "\n",
    "x_pred_coordinates_2_mu = x_pred_coordinates_2_mu_layer(decode_layer_2)\n",
    "x_pred_coordinates_2_log_var = x_pred_coordinates_2_log_var_layer(decode_layer_2)\n",
    "decode_x_pred_coordinates_2 = x_pred_coordinates_2_layer([x_pred_coordinates_2_mu, x_pred_coordinates_2_log_var])\n",
    "\n",
    "decode_x_pred_passenger_count = x_pred_passenger_count_log_lambda_layer(decode_layer_2)\n",
    "\n",
    "decode_x_pred_travel_duration = x_pred_travel_duration_log_lambda_layer(decode_layer_2)\n",
    "\n",
    "\n",
    "decoder = Model(decode_input, [decode_x_pred_coordinates, decode_x_pred_coordinates, decode_x_pred_passenger_count, decode_x_pred_travel_duration])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkmrKUgoNzuW"
   },
   "source": [
    "### Saving the Models and Metadata\n",
    "#### Saving Models (Actually, only the decoder matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "OFQM_LGxNzuX",
    "outputId": "fb960329-ce96-4962-d88e-19c3664cd726"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-43d9c1effe02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model/vae_full_model.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model/vae_full_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model to disk\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \"\"\"\n\u001b[0;32m-> 1293\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m     return json.dumps(\n\u001b[1;32m   1295\u001b[0m         model_config, default=serialization.get_json_type, **kwargs)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1272\u001b[0m     model_config = {\n\u001b[1;32m   1273\u001b[0m         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_network_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mget_network_config\u001b[0;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[1;32m   2112\u001b[0m           \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2114\u001b[0;31m     \u001b[0mlayer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2115\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m     \u001b[0mlayer_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inbound_nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    273\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    274\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mlayer_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m       \u001b[0mlayer_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;31m# When constructed using an `InputLayer` the first non-input layer may not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;31m# have the shape information to reconstruct `Sequential` as a graph network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    273\u001b[0m         return serialize_keras_class_and_config(\n\u001b[1;32m    274\u001b[0m             name, {_LAYER_UNDEFINED_CONFIG_KEY: True})\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0mserialization_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mserialize_keras_object\u001b[0;34m(instance)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_registered_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_SKIP_FAILED_SERIALIZATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m       raise NotImplementedError('Layer %s has arguments in `__init__` and '\n\u001b[1;32m    635\u001b[0m                                 \u001b[0;34m'therefore must override `get_config`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                                 self.__class__.__name__)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Layer KLDivergenceAddLoss has arguments in `__init__` and therefore must override `get_config`."
     ]
    }
   ],
   "source": [
    "model_json = vae.to_json()\n",
    "with open(\"./model/vae_full_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "vae.save_weights(\"./model/vae_full_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "encoder = Model(x, [z_mu, z_log_var])\n",
    "model_json = encoder.to_json()\n",
    "with open(\"./model/vae_encoder.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "encoder.save_weights(\"./model/vae_encoder.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "model_json = decoder.to_json()\n",
    "with open(\"./model/vae_decoder.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "decoder.save_weights(\"./model/vae_decoder.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWiUvwCeNzu2"
   },
   "source": [
    "#### Generating and Saving Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5yIjpRoNzu2"
   },
   "outputs": [],
   "source": [
    "vae_sample = dataset[np.random.choice(len(dataset), size=14000, replace=False)]\n",
    "metadata = encoder.predict(vae_sample, batch_size=batch_size)\n",
    "np.save('metadata.npy', metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsLPB5nrNzu-"
   },
   "source": [
    "### Generating some Samples for Testing\n",
    "#### Functions for data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y11mkFXCNzvB"
   },
   "outputs": [],
   "source": [
    "# reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "# def sample(model, input_mu, input_log_var, samples_per_z=1):\n",
    "    # multiplied_input_mu = np.repeat(input_mu, samples_per_z, axis=0)\n",
    "    # multiplied_input_log_var = np.repeat(input_log_var, samples_per_z, axis=0)\n",
    "    # eps = np.random.normal(size=(multiplied_input_mu.shape[0], latent_dim))\n",
    "    # z = reparameterize(multiplied_input_mu, multiplied_input_log_var, eps)\n",
    "    # predictions = model.predict(z, batch_size = None, steps = 1)\n",
    "    # return reconstruct(predictions)\n",
    "\n",
    "reverse_categorical_map = {0:0, 1:0.5, 2:1, 3:1.5, 4:2, 5:2.5, 6:3, 7:3.5, 8:4, 9:4.5, 10:5}\n",
    "def sample(decoder, sampler, input_z_params,samples_per_z=1):\n",
    "    multiplied_input_z_params = np.repeat(input_z_params, samples_per_z, axis=0)\n",
    "    z = sampler.predict(multiplied_input_z_params)\n",
    "    predictions = decoder.predict(z, batch_size = None, steps = 1)\n",
    "    return reconstruct(predictions)\n",
    "    \n",
    "def reconstruct(predictions):\n",
    "    coordinates, coordinates_2, passenger_count, travel_duration= predictions\n",
    "    mu, log_var = np.split(coordinates, indices_or_sections = 2,axis = 1)\n",
    "    eps = np.random.normal(size=mu.shape)\n",
    "    \n",
    "    \n",
    "    ## coordinates handled here\n",
    "    coordinates_data = reparameterize(mu, log_var, eps)\n",
    "    pickup_scaler = load(open('pick_up_scaler.pkl', 'rb'))\n",
    "    coordinates_data = pickup_scaler.inverse_transform(coordinates_data)\n",
    "    \n",
    "    for i, c in enumerate(coordinates_data):\n",
    "        lon = coordinates_data[i][0]\n",
    "        lat = coordinates_data[i][1]\n",
    "        x, y = meters2lonlat(lon, lat)\n",
    "        coordinates_data[i][0] = x\n",
    "        coordinates_data[i][1] = y\n",
    "\n",
    "    for i, c in enumerate(coordinates_data):\n",
    "        if c[0] > 180.0:\n",
    "            coordinates_data[i][0]= 180.0\n",
    "        if c[0] < -180.0:\n",
    "            coordinates_data[i][0]= -180.0\n",
    "        if c[1] > 180.0:\n",
    "            coordinates_data[i][1]= 180.0\n",
    "        if c[1] < -180.0:\n",
    "            coordinates_data[i][1]= -180.0\n",
    "    \n",
    "    ## coordinates handled here\n",
    "    mu, log_var = np.split(coordinates_2, indices_or_sections = 2,axis = 1)\n",
    "    eps = np.random.normal(size=mu.shape)\n",
    "\n",
    "\n",
    "    coordinates_2_data = reparameterize(mu, log_var, eps)\n",
    "    dropoff_scaler = load(open('drop_off_scaler.pkl', 'rb'))\n",
    "    coordinates_2_data = dropoff_scaler.inverse_transform(coordinates_2_data)\n",
    "    \n",
    "    for i, c in enumerate(coordinates_2_data):\n",
    "        lon = coordinates_2_data[i][0]\n",
    "        lat = coordinates_2_data[i][1]\n",
    "        x, y = meters2lonlat(lon, lat)\n",
    "        coordinates_2_data[i][0] = x\n",
    "        coordinates_2_data[i][1] = y\n",
    "    \n",
    "    for i, c in enumerate(coordinates_2_data):\n",
    "        if c[0] > 180.0:\n",
    "            coordinates_2_data[i][0]= 180.0\n",
    "        if c[0] < -180.0:\n",
    "            coordinates_2_data[i][0]= -180.0\n",
    "        if c[1] > 180.0:\n",
    "            coordinates_2_data[i][1]= 180.0\n",
    "        if c[1] < -180.0:\n",
    "            coordinates_2_data[i][1]= -180.0\n",
    "    \n",
    "    ## passenger_count handled here\n",
    "    exp_log_passenger_count = np.exp(passenger_count)\n",
    "    passenger_count_data = np.random.poisson(lam=exp_log_passenger_count, size = passenger_count.shape)\n",
    "    for i, r in enumerate(passenger_count_data):\n",
    "        if r[0] < 0:\n",
    "            passenger_count_data[i][0] = 0\n",
    "        passenger_count_data[i][0] = float(int(passenger_count_data[i][0]))\n",
    "\n",
    "    ## review_count handled here\n",
    "    exp_log_travel_duration = np.exp(travel_duration)\n",
    "    travel_duration_data = np.random.poisson(lam=exp_log_travel_duration, size = travel_duration.shape)\n",
    "    for i, r in enumerate(travel_duration_data):\n",
    "        if r[0] < 0:\n",
    "            travel_duration_data[i][0] = 0\n",
    "        travel_duration_data[i][0] = float(int(travel_duration_data[i][0]))\n",
    "        # print(travel_duration_data[i][0] )\n",
    "    \n",
    "    return np.concatenate([coordinates_data, coordinates_2_data, passenger_count_data, travel_duration_data], axis = 1)\n",
    "    \n",
    "\n",
    "def reparameterize(input_mu, input_log_var, eps):\n",
    "    sigma = np.exp(0.5*input_log_var)\n",
    "    return eps*sigma + input_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkvHLu19Nzvc"
   },
   "source": [
    "### Generating Samples\n",
    "Make use of the funciton sample to generate samples with our model. U need to supply an array of mu and their respective log var in a separate array to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fCtpUzCONzvf",
    "outputId": "b24db122-80af-44ac-a4d6-4072a5894cb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyc_vae_14000_times_5.csv\n"
     ]
    }
   ],
   "source": [
    "vae_samples = sample(decoder, sampler, metadata, 5)\n",
    "\n",
    "# # Saving the samples in a separate file\n",
    "file_name = 'nyc_vae_' + str(len(metadata))+'_times_'+ str(len(vae_samples)//len(metadata)) + '.csv'\n",
    "np.savetxt(file_name, vae_samples, delimiter = ',', header='pickup_longitude,pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count,trip_duration')\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "0xrZ7s3INzv3",
    "outputId": "93852842-4edb-4bb4-b0fc-3d53ef469422"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.39849722e+01,  4.07893836e+01, -7.39772578e+01,\n",
       "         4.07514487e+01,  3.00000000e+00,  4.23000000e+02],\n",
       "       [-7.39851313e+01,  4.07656644e+01, -7.39737269e+01,\n",
       "         4.07492343e+01,  2.00000000e+00,  4.34000000e+02],\n",
       "       [-7.40223500e+01,  4.07647697e+01, -7.39303982e+01,\n",
       "         4.08002151e+01,  3.00000000e+00,  6.04000000e+02],\n",
       "       [-7.39594563e+01,  4.07525196e+01, -7.39283271e+01,\n",
       "         4.07627181e+01,  0.00000000e+00,  9.32000000e+02],\n",
       "       [-7.39431459e+01,  4.07113147e+01, -7.40231765e+01,\n",
       "         4.07440255e+01,  0.00000000e+00,  6.15000000e+02],\n",
       "       [-7.39852891e+01,  4.07081859e+01, -7.39571242e+01,\n",
       "         4.07184679e+01,  4.00000000e+00,  3.05000000e+02],\n",
       "       [-7.39460589e+01,  4.07609297e+01, -7.39663621e+01,\n",
       "         4.07680510e+01,  1.00000000e+00,  9.75000000e+02],\n",
       "       [-7.39961156e+01,  4.07395104e+01, -7.38964835e+01,\n",
       "         4.07732447e+01,  1.00000000e+00,  1.29700000e+03],\n",
       "       [-7.40030389e+01,  4.07377161e+01, -7.39624891e+01,\n",
       "         4.07920042e+01,  2.00000000e+00,  2.18000000e+02],\n",
       "       [-7.39484611e+01,  4.07598137e+01, -7.39224966e+01,\n",
       "         4.07669131e+01,  1.00000000e+00,  1.54800000e+03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_samples[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KL2xv-RGNzv8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "NYC_VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
