{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40w5QQEBMJ01"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imageio\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython import display\n",
    "from sklearn import preprocessing\n",
    "from pickle import dump, load\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from IPython.display import SVG\n",
    "\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WVWys12NwuL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def lonlat2meters(lon, lat):\n",
    "    semimajoraxis = 6378137.0\n",
    "    east = lon * 0.017453292519943295\n",
    "    north = lat * 0.017453292519943295\n",
    "    t = math.sin(north)\n",
    "    return semimajoraxis * east, 3189068.5 * math.log((1 + t) / (1 - t))\n",
    "\n",
    "def meters2lonlat(x, y):\n",
    "    semimajoraxis = 6378137.0\n",
    "    lon = x / semimajoraxis / 0.017453292519943295\n",
    "    t = math.exp(y / 3189068.5)\n",
    "    lat = math.asin((t - 1) / (t + 1)) / 0.017453292519943295\n",
    "    return lon, lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9zGqOzTN0Ld"
   },
   "outputs": [],
   "source": [
    "dataset = np.genfromtxt('../processed_nyc_train.csv',delimiter=',', skip_header=1)\n",
    "dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "\n",
    "def format_data(dataset, pick_up_scaler=None, drop_off_scaler = None ,save_scaler=True):\n",
    "    \n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # Handling of the coordinates\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "        lon = pick_up_c[i][0]\n",
    "        lat = pick_up_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        pick_up_c[i][0] = x\n",
    "        pick_up_c[i][1] = y\n",
    "    \n",
    "    if pick_up_scaler is None:\n",
    "        pick_up_scaler = preprocessing.StandardScaler()\n",
    "        pick_up_scaler = pick_up_scaler.fit(pick_up_c)\n",
    "    \n",
    "    pick_up_c = pick_up_scaler.transform(pick_up_c)\n",
    "    \n",
    "    for i, c in enumerate(drop_off_c):\n",
    "        lon = drop_off_c[i][0]\n",
    "        lat = drop_off_c[i][1]\n",
    "        x, y = lonlat2meters(lon, lat)\n",
    "        drop_off_c[i][0] = x\n",
    "        drop_off_c[i][1] = y\n",
    "    \n",
    "    if drop_off_scaler is None:\n",
    "        drop_off_scaler = preprocessing.StandardScaler()\n",
    "        drop_off_scaler = drop_off_scaler.fit(drop_off_c)\n",
    "    \n",
    "    drop_off_c = drop_off_scaler.transform(drop_off_c)\n",
    "    \n",
    "    \n",
    "    if save_scaler:\n",
    "        dump(pick_up_scaler, open('pick_up_scaler.pkl', 'wb'))\n",
    "        dump(drop_off_scaler, open('drop_off_scaler.pkl', 'wb'))\n",
    "\n",
    "    final = np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)\n",
    "    return final\n",
    "\n",
    "dataset = format_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "iJsjGeFkMJ6Q",
    "outputId": "1d352b41-aacb-45ba-f49b-ad4c22a1ba9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow:  2.1.0\n",
      "tensorflow-probability:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization, ReLU, Conv2D, Reshape\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "tfk = tf.keras\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print('tensorflow: ', tf.__version__)\n",
    "print('tensorflow-probability: ', tfp.__version__)\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Case(Enum):\n",
    "    sampling = 1\n",
    "    density_estimation = 2\n",
    "\n",
    "class NN(Layer):\n",
    "    \"\"\"\n",
    "    Neural Network Architecture for calcualting s and t for Real-NVP\n",
    "    \n",
    "    :param input_shape: shape of the data coming in the layer\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "    :param activation: Activation of the hidden units\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_hidden=[512, 512], activation=\"relu\"):\n",
    "        super(NN, self).__init__()\n",
    "        layer_list = []\n",
    "        for i, hidden in enumerate(n_hidden):\n",
    "            layer_list.append(Dense(hidden, activation=activation))\n",
    "        self.layer_list = layer_list\n",
    "        self.log_s_layer = Dense(input_shape, activation=\"tanh\", name='log_s')\n",
    "        self.t_layer = Dense(input_shape, name='t')\n",
    "\n",
    "    def call(self, x):\n",
    "        y = x\n",
    "        for layer in self.layer_list:\n",
    "            y = layer(y)\n",
    "        log_s = self.log_s_layer(y)\n",
    "        t = self.t_layer(y)\n",
    "        return log_s, t\n",
    "\n",
    "class RealNVP(tfb.Bijector):\n",
    "    \"\"\"\n",
    "    Implementation of a Real-NVP for Denisty Estimation. L. Dinh “Density estimation using Real NVP,” 2016.\n",
    "    This implementation only works for 1D arrays.\n",
    "    :param input_shape: shape of the data coming in the layer\n",
    "    :param hidden_units: Python list-like of non-negative integers, specifying the number of units in each hidden layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, n_hidden=[512, 512], forward_min_event_ndims=1, validate_args: bool = False, name=\"real_nvp\"):\n",
    "        super(RealNVP, self).__init__(\n",
    "            validate_args=validate_args, forward_min_event_ndims=forward_min_event_ndims, name=name\n",
    "        )\n",
    "\n",
    "        assert input_shape % 2 == 0\n",
    "        input_shape = input_shape // 2\n",
    "        nn_layer = NN(input_shape, n_hidden)\n",
    "        x = tf.keras.Input(input_shape)\n",
    "        log_s, t = nn_layer(x)\n",
    "        self.nn = Model(x, [log_s, t])\n",
    "        \n",
    "    def _bijector_fn(self, x):\n",
    "        log_s, t = self.nn(x)\n",
    "        return tfb.affine_scalar.AffineScalar(shift=t, log_scale=log_s)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
    "        y_b = x_b\n",
    "        y_a = self._bijector_fn(x_b).forward(x_a)\n",
    "        y = tf.concat([y_a, y_b], axis=-1)\n",
    "        return y\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
    "        x_b = y_b\n",
    "        x_a = self._bijector_fn(y_b).inverse(y_a)\n",
    "        x = tf.concat([x_a, x_b], axis=-1)\n",
    "        return x\n",
    "\n",
    "    def _forward_log_det_jacobian(self, x):\n",
    "        x_a, x_b = tf.split(x, 2, axis=-1)\n",
    "        return self._bijector_fn(x_b).forward_log_det_jacobian(x_a, event_ndims=1)\n",
    "    \n",
    "    def _inverse_log_det_jacobian(self, y):\n",
    "        y_a, y_b = tf.split(y, 2, axis=-1)\n",
    "        return self._bijector_fn(y_b).inverse_log_det_jacobian(y_a, event_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCvWxSDNOvcg"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layers = 12\n",
    "# base_lr = 1e-3\n",
    "# end_lr = 1e-4\n",
    "# max_epochs = int(100)\n",
    "shape = [128, 128]\n",
    "exp_number = 1\n",
    "input_shape = 6\n",
    "permutation = tf.cast(np.concatenate((np.arange(input_shape/2,input_shape),np.arange(0,input_shape/2))), tf.int32)\n",
    "event_shape = [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOvfYzKMOaak"
   },
   "outputs": [],
   "source": [
    "base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(shape=input_shape, dtype=tf.float32))\n",
    "\n",
    "bijectors = []\n",
    "alpha = 1e-3\n",
    "\n",
    "for i in range(layers):\n",
    "    bijectors.append(tfb.BatchNormalization())\n",
    "    bijectors.append(RealNVP(input_shape=input_shape, n_hidden=shape))\n",
    "    bijectors.append(tfp.bijectors.Permute(permutation))\n",
    "\n",
    "\n",
    "bijector = tfb.Chain(bijectors=list(reversed(bijectors)), name='chain_of_real_nvp')\n",
    "\n",
    "flow = tfd.TransformedDistribution(\n",
    "    distribution=base_dist,\n",
    "    bijector=bijector\n",
    ")\n",
    "\n",
    "# number of trainable variables\n",
    "n_trainable_variables = len(flow.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6OeXZwdP7u4"
   },
   "outputs": [],
   "source": [
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "max_epochs = int(5e3)  # maximum number of epochs of the training\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, max_epochs, end_lr, power=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "GRby0dMrSz1B",
    "outputId": "9e53d636-f119-4f5d-a3b3-03ef07bfd057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-166a63a340c6>:72: AffineScalar.__init__ (from tensorflow_probability.python.bijectors.affine_scalar) is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`AffineScalar` bijector is deprecated; please use `tfb.Shift(loc)(tfb.Scale(...))` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/bijectors/batch_normalization.py:207: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate_fn) \n",
    "\n",
    "x_ = tf.keras.layers.Input(shape=event_shape, dtype=tf.float32)\n",
    "log_prob_ = flow.log_prob(x_)\n",
    "model = tf.keras.Model(x_, log_prob_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNwEpiz7Vwcc"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam() ,\n",
    "              loss=lambda _, log_prob: -log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Checkpoint ( Will Save Model After Every Epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKIR7DukS0XH"
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "weight_file = './checkpoint/cp.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "-OHsysMCUY7M",
    "outputId": "92004d2b-3b24-4034-b544-696bd77d17d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0177Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0217Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0228Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0239Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0247Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0248Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0152Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0153Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0250Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0217Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0162Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0142Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0163Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0249Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0264Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0231Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0166Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0223Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0276Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0281Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0242Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0238Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0202Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0252Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0194Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0230Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -250.9707Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0183Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0273Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0278Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0257Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0198Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0232Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0035Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0267Train on 1458643 samples\n",
      "1458600/1458643 [============================>.] - ETA: 0s - loss: -251.0293Train on 1458643 samples\n",
      "1396800/1458643 [===========================>..] - ETA: 34s - loss: -251.0287"
     ]
    }
   ],
   "source": [
    "for i in range(100): # set to the desired epoch\n",
    "  model.load_weights(weight_file)\n",
    "  model.fit(x=dataset,\n",
    "            y=np.zeros((dataset.shape[0], 0), dtype=np.float32),\n",
    "            batch_size=batch_size,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=dataset.shape[0] // batch_size,\n",
    "            shuffle=True,\n",
    "            verbose=True)\n",
    "  model.save_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TzQhZTTU3Sj"
   },
   "outputs": [],
   "source": [
    "model.load_weights(weight_file)\n",
    "model.save_weights(\"RNVP_12_128_128.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwNcq369U_D2"
   },
   "outputs": [],
   "source": [
    "output = flow.sample(100000).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wncLFKAQVB6f"
   },
   "outputs": [],
   "source": [
    "def reconstruct(predictions):\n",
    "    \n",
    "    # split the output first\n",
    "    pick_up_c, drop_off_c, num_passenger, travel_duration = np.split(dataset, [2, 4, 5], axis = 1)\n",
    "    \n",
    "    # recover scaler\n",
    "    pick_up_scaler = load(open('pick_up_scaler.pkl', 'rb'))\n",
    "    drop_off_scaler = load(open('drop_off_scaler.pkl', 'rb'))\n",
    "    pick_up_c = pick_up_scaler.inverse_transform(pick_up_c)\n",
    "    drop_off_c = drop_off_scaler.inverse_transform(drop_off_c)\n",
    "\n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = pick_up_c[i][0]\n",
    "      y = pick_up_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      pick_up_c[i][0] = lon\n",
    "      pick_up_c[i][1] = lat\n",
    "    \n",
    "    for i, c in enumerate(pick_up_c):\n",
    "      x = drop_off_c[i][0]\n",
    "      y = drop_off_c[i][1]\n",
    "      lon, lat = meters2lonlat(x, y)\n",
    "      drop_off_c[i][0] = lon\n",
    "      drop_off_c[i][1] = lat\n",
    "    \n",
    "    return np.concatenate([pick_up_c, drop_off_c, num_passenger, travel_duration], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDDYJY6VVKLI"
   },
   "outputs": [],
   "source": [
    "samples = reconstruct(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Generated Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arxV_BtoVMNc"
   },
   "outputs": [],
   "source": [
    "file_name = '100000_samples_RNVP_12_128_128' + '.csv'\n",
    "np.savetxt(file_name, samples, delimiter = ',', header='pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, trip_duration' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1w3U15vB3IDs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NYC_RNVP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
